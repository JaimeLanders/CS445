from mnist import MNIST
import numpy as np
import random
import sys
#np.set_printoptions(threshold=sys.maxsize) # Print full matrix
#np.set_printoptions(edgeitems=90)
#np.core.arrayprint._line_width = 180
NINPUTS = 785
NOUTPUTS = 10
TACCURACY = 0.01
NEPOCHS = 70


def main():
    print("Welcome to HW1!")


    # Load MNIST data
    # Source: https://pypi.org/project/python-mnist/
    mndata = MNIST('./')
    mndata.gz = True
    images, labels = mndata.load_training()

#    print(images)
#    print(type(images))
#    print(len(images))

    # Convert to NumPy matrices
    nimages = np.asarray(images)
#    print("\nnimages = ", nimages)
    print("nimages dim = ", nimages.ndim)
    print("nimages shape = ", nimages.shape)
    print("nimages size = ", nimages.size)

#    print(labels)
#    print(type(labels))
#    print(len(labels))

    nlabels = np.asarray(labels)
#    print("\nnlabels = ", nlabels)
    print("nlabels dim = ", nlabels.ndim)
    print("nlabels shape = ", nlabels.shape)
    print("nlabels size = ", nlabels.size)

    # Preprocess data
    nimages = nimages / 255
    print("\nprocessed nimages = ", nimages)
#    print("\nnimages[0] = ", nimages[0])

    # Train perceptrons with three different learning rates
    η1 = 0.001
    η2 = 0.01
    η3 = 0.1

#    pla(nimages, nlabels, η1)
#    pla(nimages, nlabels, η2)
    pla(nimages, nlabels, η3)


def pla(nimages, nlabels, η):
    print("\npla()")
    print("η = ", η)

    #Start with small random weights, w = (w0, w1, w2, ... , wn), where wi E [-.05, .05]
    a = -0.5
    b = 0.5
#    w = (b - a) * np.random.rand(NINPUTS, 1) + a
    w = np.ones((NINPUTS))

#    print("w = ", w)
    print("w dim = ", w.ndim)
    print("w shape = ", w.shape)
    print("w size = ", w.size)


    for i in range(1, w.shape[0]):
        w[i] = random.uniform(-0.5, 0.5)
#        w[i] = random.randfloat(-0.5, 0.5)
#    print("w random = ", w)

    M = nimages.shape[0]
    print("M = ", M)

    t = nlabels
#    t = np.zeros((M, 1))
    print("t = ", t)
    print("t dim = ", t.ndim)
    print("t shape = ", t.shape)
    print("t size = ", t.size)

    # Repeat until accuracy on the training data stops increasing or for a maximum number of epochs (iterations through training set)
    # For k = 1 to M(total number in training set
    for k in range(0, M):
    #        print(k)
        # Select next training example (x^k, t^k)
#        xk = np.zeros((1))
        xk = np.array([1])
#        xk = nimages[k]
#        np.append(xk, [1])
        tarray = nimages[k]
        xk = np.append(xk, tarray)
#        print("xk = ", xk)
#        print("xk dim = ", xk.ndim)
#        print("xk shape = ", xk.shape)
#        print("xk size = ", xk.size)

        tk = t[k]
#        print("t^k = ", tk)

        # Run the perceptron with input xkand weights wto obtain y.
        y = perceptron(xk, w)
#        print("y = ", y)

        # If y != t^k, update weights:
        if y != tk:
#            print("y != t^k")
            for i in range(w.shape[0]):
                ti = 0
                yi = 0
                if y > 0:
                    yi = 1
                Δwi = η * (yi - ti) * xk[i]
#                print("Δwi = ", Δwi)
                w[i] = w[i] + Δwi
#                print("w[i] = ", w[i])


def perceptron(xk, w):
#    print("\nperceptron()")
#    print("xk = ", xk)
#    print("w = ", w)

    y = np.dot(xk, w)

    return y

if __name__ == '__main__':
    main()